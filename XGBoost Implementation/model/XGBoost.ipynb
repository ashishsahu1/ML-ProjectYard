{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from asciitree import LeftAligned\nfrom collections import OrderedDict as OD\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom jax import grad, jacfwd, jacrev, jit\nimport jax.numpy as jnp\nimport numpy as np\nfrom drawtree import draw_level_order\n\nimport random\n\ndef hessian(fun):\n  return jit(jacfwd(jacrev(fun)))\n\nclass DecisionNode:\n    \"\"\"\n    Node decision class.\n    This is a simple binary node, with potentially two childs: left and right\n    Left node is returned when condition is true\n    False node is returned when condition is false<\n    \"\"\"\n    def __init__(self, name, condition, value=None):\n        self.name = name\n        self.condition = condition\n        self.value = value\n        self.left = None\n        self.right = None\n\n    def add_left_node(self, left):\n        self.left = left\n\n    def add_right_node(self, right):\n        self.right = right\n\n    def is_leaf(self):\n        \"\"\"\n        Node is a leaf if it has no child\n        \"\"\"\n        return (not self.left) and (not self.right)\n\n    def next(self, data):\n        \"\"\"\n        Return next code depending on data and node condition\n        \"\"\"\n        cond = self.condition(data)\n        if cond:\n            return self.left\n        else:\n            return self.right\n\nclass DecisionTree:\n    \"\"\"\n    A DecisionTree is a model that provides predictions depending on input.\n    Prediction is the sum of the values attached to leaf activated by input\n    \"\"\"\n    def __init__(self, objective, nb_estimators, max_depth):\n        \"\"\"\n        A DecisionTree is defined by an objective, a number of estimators and a max depth.\n        \"\"\"\n        self.roots = [DecisionNode(f'root_{esti}', None, 0.0) for esti in range(0, nb_estimators)]\n        self.objective = objective\n        self.lbda = 0.0\n        self.gamma = 1.0 * 0\n        self.grad = grad(self.objective)\n        self.hessian = hessian(self.objective)\n        self.max_depth = max_depth\n        self.base_score = None\n\n\n    def _create_condition(self, col_name, split_value):\n        \"\"\"\n        Create a closure that capture split value\n        \"\"\"\n        return lambda dta : dta[col_name] < split_value\n\n    def _pick_columns(self, columns):\n        return random.choice(columns)\n\n    def _add_child_nodes(self, node, nodes,\n                         node_x, node_y,\n                         split_value, split_column,\n                         nb_nodes,\n                         left_w, right_w, prev_w):\n        node.name = f'{split_column} < {split_value}'\n        node.condition = self._create_condition(split_column, split_value) # we must create a closure to capture split_value copy\n        node.add_left_node(DecisionNode(f'left_{nb_nodes} - {split_column} < {split_value}',\n                                        None, left_w + prev_w))\n        node.add_right_node(DecisionNode(f'right_{nb_nodes} - {split_column} >= {split_value}',\n                                         None, right_w + prev_w))\n        mask = node_x[split_column] < split_value\n        # Reverse order to ensure bfs\n        nodes.append((node.left,\n                      node_x[mask].copy(),\n                      node_y[mask].copy(),\n                      left_w + prev_w))\n        nodes.append((node.right,\n                      node_x[~mask].copy(),\n                      node_y[~mask].copy(),\n                      right_w + prev_w))\n\n\n    def fit(self, x_train, y_train):\n        \"\"\"\n        Fit decision trees using x_train and objective\n        \"\"\"\n        self.base_score = y_train.mean()\n        for tree_idx, tree_root in enumerate(self.roots):\n            # store current node (currenly a lead), x_train and node leaf weight\n            nodes = [(tree_root, x_train.copy(), y_train.copy(), 0.0)]\n            nb_nodes = 0\n            # Add node to tree using bfs\n            while nodes:\n                node, node_x, node_y, prev_w = nodes.pop(0)\n                node_x['pred'] = self.predict(node_x)\n                split_column = self._pick_columns(x_train.columns) # XGBoost use a smarter heuristic here\n                best_split, split_value, left_w, right_w = self._find_best_split(split_column,\n                                                                                 node_x, node_y)\n                if best_split != -1:\n                    self._add_child_nodes(node, nodes,\n                                          node_x, node_y,\n                                          split_value, split_column,\n                                          nb_nodes,\n                                          left_w, right_w, prev_w)\n                nb_nodes += 1\n                if nb_nodes >= 2**self.max_depth-1:\n                    break\n\n\n    def _gain_and_weight(self, x_train, y_train):\n        \"\"\"\n        Compute gain and leaf weight using automatic differentiation\n        \"\"\"\n        pred = x_train['pred'].values\n        G_i = self.grad(pred, y_train.values).sum()\n        H_i = self.hessian(pred, y_train.values).sum()\n        return -0.5 * G_i * G_i / (H_i + self.lbda) + self.gamma, -G_i / (H_i + self.lbda)\n\n    def _find_best_split(self, col_name, node_x, node_y):\n        \"\"\"\n        Compute best split\n        \"\"\"\n        x_sorted = node_x.sort_values(by=col_name)\n        y_sorted = node_y[x_sorted.index]\n        current_gain, _ = self._gain_and_weight(x_sorted, node_y)\n        gain = 0.0\n        best_split = -1\n        split_value, best_left_w, best_right_w = None, None, None\n        for split_idx in range(1, x_sorted.shape[0]):\n            left_data = x_sorted.iloc[:split_idx]\n            right_data = x_sorted.iloc[split_idx:]\n            left_y = y_sorted.iloc[:split_idx]\n            right_y = y_sorted.iloc[split_idx:]\n            left_gain, left_w = self._gain_and_weight(left_data, left_y)\n            right_gain, right_w = self._gain_and_weight(right_data, right_y)\n            if current_gain - (left_gain + right_gain) > gain:\n                gain = current_gain - (left_gain + right_gain)\n                best_split = split_idx\n                split_value = x_sorted[col_name].iloc[split_idx]\n                best_left_w = left_w\n                best_right_w = right_w\n        return best_split, split_value, best_left_w, best_right_w\n\n    def predict(self, data):\n        preds = []\n        for _, row in data.iterrows():\n            pred = 0.0\n            for tree_idx, root in enumerate(self.roots):\n                child = root\n                while child and not child.is_leaf():\n                    child = child.next(row)\n                pred += child.value\n            preds.append(pred)\n        return np.array(preds) + self.base_score\n\n    def show(self):\n        print('not yet implemented')\n\n\ndef squared_error(y_pred, y_true):\n    diff = y_true - y_pred\n    return jnp.dot(diff, diff.T)\n\nx_train = pd.DataFrame({\"A\" : [3.0, 2.0, 1.0, 4.0, 5.0, 6.0, 7.0]})\ny_train = pd.DataFrame({\"Y\" : [3.0, 2.0, 1.0, 4.0, 5.0, 6.0, 7.0]})\n\ntree = DecisionTree(squared_error, 1, 3)\ntree.fit(x_train, y_train['Y'])\npred = tree.predict(pd.DataFrame({'A': [1., 2., 3., 4., 5., 6., 7.]}))\nprint(pred) #-> [1. 2. 3. 4. 5. 6. 7.]\n\ntree = DecisionTree(squared_error, 2, 3)\ntree.fit(x_train, y_train['Y'])\npred = tree.predict(pd.DataFrame({'A': [1., 2., 3., 4., 5., 6., 7.]}))\nprint(pred) #-> [1. 2. 3. 4. 5. 6. 7.]\n\ntree = DecisionTree(squared_error, 4, 2)\ntree.fit(x_train, y_train['Y'])\npred = tree.predict(pd.DataFrame({'A': [1., 2., 3., 4., 5., 6., 7.]}))\nprint(pred) # -> [1.        2.        3.        4.        5.        5.9999995 7.       ]\n\nx_train = pd.DataFrame({'A': [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0,\n                              1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0,],\n                        'B': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                              1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,]})\ny_train = pd.DataFrame({\"Y\" : [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0,\n                               1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5]})\n\ntree = DecisionTree(squared_error, 1, 6)\ntree.fit(x_train, y_train['Y'])\npred = tree.predict(pd.DataFrame({'A': [1., 2., 3., 4., 5., 6., 7.],\n                                  'B': [0., 1., 0., 1., 0., 1., 0.]}))\nprint(pred) #-> [1.  2.5 3.  4.5 5.  6.5 7. ]","execution_count":1,"outputs":[{"output_type":"stream","text":"WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n","name":"stderr"},{"output_type":"stream","text":"[1. 2. 3. 4. 5. 6. 7.]\n[1. 2. 3. 4. 5. 6. 7.]\n[1.        2.        3.        4.        5.        5.9999995 7.       ]\n[1.  2.5 3.  4.5 5.  6.5 7. ]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":5}