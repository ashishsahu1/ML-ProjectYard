{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPry17rx98mk2VXp7g9dcOs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salonidabgar/GSSOC-21/blob/main/UNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhbC0NdkHJmf",
        "outputId": "85946aac-4b47-4df5-a1b5-966ffefc9926"
      },
      "source": [
        "!pip install torch==1.8.1+cu102 torchvision==0.9.1+cu102 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torchvision"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.1+cu102\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu102/torch-1.8.1%2Bcu102-cp37-cp37m-linux_x86_64.whl (804.1MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1MB 19kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.1+cu102\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu102/torchvision-0.9.1%2Bcu102-cp37-cp37m-linux_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 231kB/s \n",
            "\u001b[?25hCollecting torchaudio===0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/55/01ad9244bcd595e39cea5ce30726a7fe02fd963d07daeb136bfe7e23f0a5/torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu102) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu102) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1+cu102) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "Successfully installed torch-1.8.1+cu102 torchaudio-0.8.1 torchvision-0.9.1+cu102\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu102)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.8.1+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UT9XFgnHL-6"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ztAk_27IJWy"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ec-s_prJERW"
      },
      "source": [
        "def double_conv(in_c , out_c):\n",
        "  conv = nn.Sequential(\n",
        "      nn.Conv2d(in_c , out_c , kernel_size = 3),\n",
        "      nn.ReLU(inplace = True),\n",
        "      nn.Conv2d(out_c , out_c , kernel_size = 3),\n",
        "      nn.ReLU(inplace = True),\n",
        "  ) \n",
        "  return conv"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJxQHZWoP1C3"
      },
      "source": [
        "def crop_image( tensor , target_tensor):\n",
        "  target_size = target_tensor.size()[2]\n",
        "  tensor_size = tensor.size()[2]\n",
        "  delta = tensor_size - target_size\n",
        "  delta = (delta)//2\n",
        "\n",
        "  return tensor[:, :, delta:tensor_size-delta , delta:tensor_size-delta]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V0uN8UbIOZW"
      },
      "source": [
        " class UNet(nn.Module):\n",
        "   def __init__(self):  \n",
        "     super(UNet, self).__init__()\n",
        "\n",
        "     self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "     self.down_conv_1 = double_conv(1, 64)\n",
        "     self.down_conv_2 = double_conv(64, 128)\n",
        "     self.down_conv_3 = double_conv(128, 256)\n",
        "     self.down_conv_4 = double_conv(256, 512)\n",
        "     self.down_conv_5 = double_conv(512, 1024)  \n",
        "\n",
        "     self.up_trans_1 = nn.ConvTranspose2d(in_channels = 1024 ,  \n",
        "                                          out_channels =512\n",
        "                                          , kernel_size = 2,\n",
        "                                          stride = 2)\n",
        "     self.up_conv_1 = double_conv(1024, 512)\n",
        "\n",
        "     self.up_trans_2 = nn.ConvTranspose2d(in_channels = 512 ,  \n",
        "                                          out_channels =256\n",
        "                                          , kernel_size = 2,\n",
        "                                          stride = 2)\n",
        "     self.up_conv_2 = double_conv(512,256)\n",
        "           \n",
        "     self.up_trans_3 = nn.ConvTranspose2d(in_channels = 256 ,  \n",
        "                                          out_channels =128\n",
        "                                          , kernel_size = 2,\n",
        "                                          stride = 2)\n",
        "     self.up_conv_3 = double_conv(256, 128)\n",
        "           \n",
        "\n",
        "     self.up_trans_4 = nn.ConvTranspose2d(in_channels = 128 ,  \n",
        "                                          out_channels =64\n",
        "                                          , kernel_size = 2,\n",
        "                                          stride = 2)\n",
        "     self.up_conv_4 = double_conv(128, 64)\n",
        "\n",
        "     self.out = nn.Conv2d(\n",
        "         in_channels = 64,\n",
        "         out_channels = 2,\n",
        "         kernel_size =1 \n",
        "     )\n",
        "\n",
        "\n",
        "           \n",
        "           \n",
        "    \n",
        "   def forward(self, image):\n",
        "\n",
        "      #encoder part\n",
        "      x1 = self.down_conv_1(image)#\n",
        "      x2 = self.max_pool_2x2(x1)\n",
        "      x3 = self.down_conv_2(x2)#\n",
        "      x4 = self.max_pool_2x2(x3)\n",
        "      x5 = self.down_conv_3(x4)#\n",
        "      x6 = self.max_pool_2x2(x5)\n",
        "      x7 = self.down_conv_4(x6)#\n",
        "      x8 = self.max_pool_2x2(x7)\n",
        "      x9 = self.down_conv_5(x8)\n",
        "\n",
        "      \n",
        "\n",
        "      #decoder part\n",
        "      x = self.up_trans_1(x9)\n",
        "      y = crop_image(x7,x)\n",
        "      x = self.up_conv_1(torch.cat([x ,y] ,1))\n",
        "\n",
        "      x = self.up_trans_2(x)\n",
        "      y = crop_image(x5,x)\n",
        "      x = self.up_conv_2(torch.cat([x ,y] ,1))\n",
        "\n",
        "      x = self.up_trans_3(x)\n",
        "      y = crop_image(x3,x)\n",
        "      x = self.up_conv_3(torch.cat([x ,y] ,1))\n",
        "\n",
        "      x = self.up_trans_4(x)\n",
        "      y = crop_image(x1,x)\n",
        "      x = self.up_conv_4(torch.cat([x ,y] ,1))\n",
        "\n",
        "      x = self.out(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "      print(x.size())\n",
        "      "
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqx3_V7RL-s1",
        "outputId": "e3885145-6dfa-445a-adfd-0b96652397aa"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  image = torch.rand((1, 1, 572, 572))\n",
        "  model = UNet()\n",
        "  print(model(image))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[-0.0211, -0.0273, -0.0232,  ..., -0.0152, -0.0232, -0.0261],\n",
            "          [-0.0237, -0.0231, -0.0224,  ..., -0.0226, -0.0254, -0.0258],\n",
            "          [-0.0212, -0.0228, -0.0217,  ..., -0.0206, -0.0225, -0.0248],\n",
            "          ...,\n",
            "          [-0.0197, -0.0242, -0.0220,  ..., -0.0255, -0.0227, -0.0247],\n",
            "          [-0.0209, -0.0258, -0.0258,  ..., -0.0241, -0.0279, -0.0269],\n",
            "          [-0.0263, -0.0259, -0.0237,  ..., -0.0256, -0.0264, -0.0275]],\n",
            "\n",
            "         [[-0.0462, -0.0413, -0.0438,  ..., -0.0453, -0.0449, -0.0439],\n",
            "          [-0.0421, -0.0477, -0.0414,  ..., -0.0396, -0.0425, -0.0385],\n",
            "          [-0.0408, -0.0472, -0.0475,  ..., -0.0378, -0.0380, -0.0451],\n",
            "          ...,\n",
            "          [-0.0432, -0.0425, -0.0441,  ..., -0.0414, -0.0423, -0.0446],\n",
            "          [-0.0387, -0.0433, -0.0436,  ..., -0.0467, -0.0455, -0.0482],\n",
            "          [-0.0451, -0.0410, -0.0415,  ..., -0.0438, -0.0470, -0.0408]]]],\n",
            "       grad_fn=<ThnnConv2DBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jdp2NYxMQma"
      },
      "source": [
        " "
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRg0Sl04VwhT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}